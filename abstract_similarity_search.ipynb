{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNi8FUxNGmYIVmIgDtDPx7J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pP2D_kcTC-zx"},"outputs":[],"source":["# âœ… 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ì´ˆê¸° ëª¨ë¸ ë¡œë”©\n","!pip install transformers datasets sentencepiece nltk\n","from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n","\n","model_name = \"google/pegasus-large\"\n","tokenizer = PegasusTokenizer.from_pretrained(model_name)\n","model = PegasusForConditionalGeneration.from_pretrained(model_name)"]},{"cell_type":"code","source":["# âœ… 2. ë°ì´í„° ë¡œë”©\n","!wget https://huggingface.co/datasets/gfissore/arxiv-abstracts-2021/resolve/main/arxiv-abstracts.jsonl.gz\n","import gzip\n","import shutil\n","import json\n","\n","with gzip.open('arxiv-abstracts.jsonl.gz', 'rb') as f_in:\n","    with open('arxiv-abstracts.jsonl', 'wb') as f_out:\n","        shutil.copyfileobj(f_in, f_out)\n","\n","filtered_papers = []\n","\n","with open('arxiv-abstracts.jsonl', 'r') as f:\n","    for line in f:\n","        data = json.loads(line)\n","        categories = data.get('categories', [])\n","        if any('cs.' in category for category in categories):\n","            filtered_papers.append({\n","                \"title\": data[\"title\"].strip(),\n","                \"abstract\": data[\"abstract\"].strip()\n","            })\n","\n","print(f\"ì´ {len(filtered_papers)}ê°œì˜ Computer Science ë…¼ë¬¸ì´ ë¡œë“œë¨.\")\n"],"metadata":{"id":"dbSYyAC4DGVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# âœ… 3. ì‚¬ì „í•™ìŠµ ë°ì´í„° ìƒì„±\n","from transformers import PegasusTokenizer\n","from tqdm import tqdm\n","import torch\n","import random\n","import os\n","\n","# ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# PEGASUS tokenizer ë¡œë“œ\n","tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n","\n","# ë¬¸ìž¥ ë¶„ë¦¬ + GSG-style ì „ì²˜ë¦¬\n","def simple_sentence_split(text):\n","    sentences = [s.strip() + '.' for s in text.split('.') if len(s.strip()) > 0]\n","    return sentences\n","\n","def gap_sentence_preparation_simple(text, num_sentences_to_mask=3):\n","    sentences = simple_sentence_split(text)\n","    if len(sentences) <= num_sentences_to_mask:\n","        return None\n","    selected = sorted(random.sample(range(len(sentences)), num_sentences_to_mask))\n","    target = \" \".join([sentences[i] for i in selected])\n","    source = \" \".join([sentences[i] for i in range(len(sentences)) if i not in selected])\n","    return source, target\n","\n","# ë°°ì¹˜ í† í¬ë‚˜ì´ì§•\n","def tokenize_in_batches(text_list, batch_size=128, max_length=512, is_target=False):\n","    all_input_ids = []\n","    all_attention_masks = []\n","\n","    for i in tqdm(range(0, len(text_list), batch_size)):\n","        batch = text_list[i:i+batch_size]\n","        if is_target:\n","            with tokenizer.as_target_tokenizer():\n","                tokenized = tokenizer(batch, truncation=True, padding=\"max_length\",\n","                                      max_length=max_length, return_tensors=\"pt\")\n","        else:\n","            tokenized = tokenizer(batch, truncation=True, padding=\"max_length\",\n","                                  max_length=max_length, return_tensors=\"pt\")\n","\n","        all_input_ids.append(tokenized[\"input_ids\"])\n","        all_attention_masks.append(tokenized[\"attention_mask\"])\n","\n","    return {\n","        \"input_ids\": torch.cat(all_input_ids),\n","        \"attention_mask\": torch.cat(all_attention_masks)\n","    }\n","\n","# ì „ì²´ GSG + í† í¬ë‚˜ì´ì§• + ë“œë¼ì´ë¸Œ ì €ìž¥ê¹Œì§€\n","def process_and_save_chunks(all_papers, chunk_size=10000, output_dir=\"/content/drive/MyDrive/pegasus_datas\"):\n","    os.makedirs(output_dir, exist_ok=True)\n","    chunk_id = 0\n","    train_pairs = []\n","    train_titles = []\n","\n","    for paper in all_papers:\n","        combined_text = paper[\"title\"] + \". \" + paper[\"abstract\"]\n","        result = gap_sentence_preparation_simple(combined_text)\n","        if result:\n","            train_pairs.append(result)\n","            train_titles.append(paper[\"title\"])  # âœ… ê°™ì´ ì €ìž¥\n","\n","        if len(train_pairs) == chunk_size:\n","            print(f\"\\nðŸ§© Chunk {chunk_id+1} - GSG ìŒ {len(train_pairs)}ê°œ ì²˜ë¦¬ ì¤‘\")\n","\n","            sources = [src for src, _ in train_pairs]\n","            targets = [tgt for _, tgt in train_pairs]\n","\n","            print(\"ðŸ”„ Source í† í¬ë‚˜ì´ì§• ì¤‘...\")\n","            source_inputs = tokenize_in_batches(sources, max_length=512)\n","\n","            print(\"ðŸ”„ Target (labels) í† í¬ë‚˜ì´ì§• ì¤‘...\")\n","            target_inputs = tokenize_in_batches(targets, max_length=128, is_target=True)\n","\n","            tokenized_inputs = {\n","                \"input_ids\": source_inputs[\"input_ids\"],\n","                \"attention_mask\": source_inputs[\"attention_mask\"],\n","                \"labels\": target_inputs[\"input_ids\"],\n","                \"titles\": train_titles  # âœ… ì§„ì§œ title ì¶”ê°€ ì €ìž¥\n","            }\n","\n","            save_path = os.path.join(output_dir, f\"chunk_{chunk_id:03d}.pt\")\n","            torch.save(tokenized_inputs, save_path)\n","            print(f\"âœ… ì €ìž¥ ì™„ë£Œ: {save_path}\")\n","\n","            # âœ… ì´ˆê¸°í™”\n","            train_pairs = []\n","            train_titles = []\n","            chunk_id += 1\n","\n","    print(\"\\nðŸŽ‰ ëª¨ë“  chunk ì²˜ë¦¬ ë° Google Drive ì €ìž¥ ì™„ë£Œ!\")\n","\n","\n","\n","process_and_save_chunks(filtered_papers, chunk_size=10000)"],"metadata":{"id":"nWFxBm-XDOH0","executionInfo":{"status":"aborted","timestamp":1748322634405,"user_tz":-540,"elapsed":1,"user":{"displayName":"ê¹€ë™ì¤€","userId":"00919442453400366852"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# âœ… 4. Pegasus ì¶”ê°€ pretraining\n","import os\n","import torch\n","import gc\n","from transformers import PegasusForConditionalGeneration, Trainer, TrainingArguments\n","from torch.utils.data import Dataset\n","\n","# ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#  Huggingface ë¡œê¹… ë¹„í™œì„±í™”\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","# GSG Dataset ì •ì˜\n","class GSGDataset(Dataset):\n","    def __init__(self, data):\n","        self.input_ids = data[\"input_ids\"]\n","        self.attention_mask = data[\"attention_mask\"]\n","        self.labels = data[\"labels\"]\n","\n","    def __len__(self):\n","        return self.input_ids.size(0)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"input_ids\": self.input_ids[idx],\n","            \"attention_mask\": self.attention_mask[idx],\n","            \"labels\": self.labels[idx]\n","        }\n","\n","# A100 ê¸°ì¤€ í•™ìŠµ ì¸ìž\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/pegasus_ckpt\",\n","    per_device_train_batch_size=8,          # A100ì—ì„  8ë„ ì—¬ìœ ë¡œì›€\n","    gradient_accumulation_steps=4,          # effective batch size = 32\n","    num_train_epochs=1,\n","    logging_steps=10,\n","    save_steps=999999,\n","    save_total_limit=1,\n","    fp16=True,                              # A100ì—ì„œ fp16 ì™„ë²½ ì§€ì›\n","    report_to=\"none\"\n",")\n","\n","# chunk ë””ë ‰í† ë¦¬\n","chunk_dir = \"/content/drive/MyDrive/pegasus_chunks\"\n","chunk_files = sorted([f for f in os.listdir(chunk_dir) if f.endswith(\".pt\") and \"part\" not in f])\n","\n","# 0ë²ˆ ~ 9ë²ˆ chunk (ì´ 100,000ê°œ ìƒ˜í”Œ) í•™ìŠµ\n","for chunk_id, chunk_file in enumerate(chunk_files[:10]):\n","    print(f\"\\nðŸš€ [Chunk {chunk_id+1}/10] í•™ìŠµ ì‹œìž‘: {chunk_file}\")\n","\n","    # ë¶ˆëŸ¬ì˜¤ê¸°\n","    chunk_path = os.path.join(chunk_dir, chunk_file)\n","    data = torch.load(chunk_path)\n","    dataset = GSGDataset(data)\n","\n","    # ëª¨ë¸\n","    model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\")\n","    trainer = Trainer(model=model, args=training_args, train_dataset=dataset)\n","\n","    # í•™ìŠµ\n","    trainer.train()\n","\n","    # ì €ìž¥\n","    model_path = f\"/content/drive/MyDrive/pegasus_ckpt/{chunk_file.replace('.pt', '')}\"\n","    model.save_pretrained(model_path)\n","    print(f\"âœ… ëª¨ë¸ ì €ìž¥ ì™„ë£Œ: {model_path}\")\n","\n","    # ì •ë¦¬\n","    del data\n","    del dataset\n","    del model\n","    del trainer\n","    gc.collect()"],"metadata":{"id":"wgU7foMUDRli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# âœ… 5. Pegasus ìš”ì•½ë¬¸ ìƒì„±\n","import os\n","import json\n","import torch\n","from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n","\n","# ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ëª¨ë¸ ë¡œë“œ\n","model_path = \"/content/drive/MyDrive/pegasus_ckpt/chunk_009\"\n","model = PegasusForConditionalGeneration.from_pretrained(model_path, local_files_only=True).cuda()\n","tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n","model.eval()\n","\n","# chunk ê²½ë¡œ\n","chunk_dir = \"/content/drive/MyDrive/pegasus_datas\"\n","chunk_files = sorted([\n","    f for f in os.listdir(chunk_dir)\n","    if f.endswith(\".pt\") and \"part\" not in f\n","])[10:20]\n","\n","# ì €ìž¥ ê²½ë¡œ\n","os.makedirs(\"/content/drive/MyDrive/pegasus_outputs\", exist_ok=True)\n","output_jsonl = \"/content/drive/MyDrive/pegasus_outputs/title_summary_100k.jsonl\"\n","\n","# ì¶”ë¡  ì„¤ì •\n","batch_size = 32\n","num_beams = 2\n","\n","with open(output_jsonl, \"w\", encoding=\"utf-8\") as f_out:\n","    for chunk_id, chunk_file in enumerate(chunk_files):\n","        print(f\"\\nðŸ“¦ [Chunk {chunk_id+10}] ìš”ì•½ë¬¸ ìƒì„± ì‹œìž‘: {chunk_file}\")\n","        chunk_path = os.path.join(chunk_dir, chunk_file)\n","        data = torch.load(chunk_path)\n","\n","        input_ids = data[\"input_ids\"].cuda()\n","        attention_mask = data[\"attention_mask\"].cuda()\n","        titles = data[\"titles\"]  # âœ… ì§„ì§œ íƒ€ì´í‹€ ì‚¬ìš©\n","\n","        for i in range(0, len(input_ids), batch_size):\n","            input_batch = input_ids[i:i+batch_size]\n","            attn_batch = attention_mask[i:i+batch_size]\n","            titles_batch = titles[i:i+batch_size]\n","\n","            with torch.no_grad():\n","                summary_ids = model.generate(\n","                    input_batch,\n","                    attention_mask=attn_batch,\n","                    max_length=128,\n","                    num_beams=num_beams,\n","                    no_repeat_ngram_size=3,   # âœ… ë°˜ë³µ ë°©ì§€\n","                    early_stopping=True\n","                )\n","\n","            summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n","\n","            for title, summary in zip(titles_batch, summaries):\n","                f_out.write(json.dumps({\n","                    \"title\": title,\n","                    \"summary\": summary\n","                }, ensure_ascii=False) + \"\\n\")\n","\n","        print(f\"âœ… {chunk_file} ìƒì„± ë° ì €ìž¥ ì™„ë£Œ\")\n"],"metadata":{"id":"raz-9-sHDUVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# âœ… 6. SimCSE ìž„ë² ë”© ë²¡í„° ìƒì„± (ë³„ë„ì˜ ì¶”ê°€ í•™ìŠµ ì—†ì´ ì‚¬ìš©)\n","import os\n","import json\n","import torch\n","from tqdm import tqdm\n","from transformers import AutoTokenizer, AutoModel\n","\n","# ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ëª¨ë¸ ë¡œë“œ (SimCSE base version)\n","model_name = \"princeton-nlp/sup-simcse-bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name).cuda()\n","model.eval()\n","\n","# ìš”ì•½ë¬¸ ë¡œë“œ\n","input_path = \"/content/drive/MyDrive/pegasus_outputs/title_summary_100k.jsonl\"\n","titles = []\n","summaries = []\n","\n","with open(input_path, \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        obj = json.loads(line)\n","        titles.append(obj[\"title\"])\n","        summaries.append(obj[\"summary\"])\n","\n","# ìž„ë² ë”© í•¨ìˆ˜ (batch ë‹¨ìœ„ ì²˜ë¦¬)\n","def get_embeddings(texts, batch_size=32):\n","    all_embeddings = []\n","\n","    for i in tqdm(range(0, len(texts), batch_size)):\n","        batch = texts[i:i+batch_size]\n","        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(\"cuda\")\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs, return_dict=True)\n","            embeddings = outputs.pooler_output  # [CLS] ìž„ë² ë”© (SimCSEì—ì„œëŠ” ì´ê²Œ ê¸°ì¤€)\n","            all_embeddings.append(embeddings.cpu())\n","\n","    return torch.cat(all_embeddings, dim=0)  # (N, 768)\n","\n","# ìž„ë² ë”© ìƒì„±\n","print(\"ðŸ”„ SimCSE ìž„ë² ë”© ìƒì„± ì¤‘...\")\n","summary_embeddings = get_embeddings(summaries)  # shape: (100000, 768)\n","\n","# ì €ìž¥\n","save_path = \"/content/drive/MyDrive/pegasus_outputs/simcse_summary_embeddings.pt\"\n","torch.save({\n","    \"titles\": titles,\n","    \"embeddings\": summary_embeddings\n","}, save_path)\n","\n","print(f\"âœ… ì €ìž¥ ì™„ë£Œ: {save_path} (shape: {summary_embeddings.shape})\")\n"],"metadata":{"id":"NJMqQ_w1DXZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# âœ… 7. ì‹¤ì œ ê²€ìƒ‰í•  ì´ˆë¡ì˜ ìš”ì•½ë¬¸ ë° ìž„ë² ë”© ìƒì„± í•¨ìˆ˜ ì •ì˜\n","import torch\n","import torch.nn.functional as F\n","from transformers import PegasusTokenizer, PegasusForConditionalGeneration, AutoTokenizer, AutoModel\n","from sklearn.metrics.pairwise import cosine_similarity\n","import json\n","\n","# ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ì‚¬ì „ ì¤€ë¹„: ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ & ëª¨ë¸ ë¡œë“œ\n","pegasus_path = \"/content/drive/MyDrive/pegasus_ckpt/chunk_009\"\n","simcse_path = \"/content/drive/MyDrive/pegasus_outputs/simcse_summary_embeddings.pt\"\n","\n","# PEGASUS\n","pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n","pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_path, local_files_only=True).cuda()\n","pegasus_model.eval()\n","\n","# SimCSE\n","simcse_tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n","simcse_model = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\").cuda()\n","simcse_model.eval()\n","\n","# ì €ìž¥ëœ ìž„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸°\n","data = torch.load(simcse_path)\n","all_titles = data[\"titles\"]\n","all_embeddings = data[\"embeddings\"]  # (100000, 768), CPU ìƒì— ìžˆìŒ\n","\n","# í•¨ìˆ˜ 1: PEGASUSë¡œ ìš”ì•½\n","def summarize_abstract(abstract):\n","    inputs = pegasus_tokenizer(abstract, return_tensors=\"pt\", truncation=True, padding=\"longest\", max_length=512).to(\"cuda\")\n","    with torch.no_grad():\n","        summary_ids = pegasus_model.generate(\n","            **inputs,\n","            max_length=128,\n","            num_beams=2,\n","            no_repeat_ngram_size=3,\n","            early_stopping=True\n","        )\n","    return pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","# í•¨ìˆ˜ 2: SimCSE ìž„ë² ë”© ìƒì„±\n","def get_simcse_embedding(text):\n","    inputs = simcse_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(\"cuda\")\n","    with torch.no_grad():\n","        outputs = simcse_model(**inputs)\n","        return outputs.pooler_output[0].cpu()  # (768,)\n","\n","# í•¨ìˆ˜ 3: ìœ ì‚¬ë„ top-k\n","def retrieve_top_k(query_vector, candidate_vectors, candidate_titles, top_k=20):\n","    similarities = F.cosine_similarity(query_vector.unsqueeze(0), candidate_vectors)  # (100000,)\n","    topk_indices = torch.topk(similarities, k=top_k).indices\n","    return [(candidate_titles[i], similarities[i].item()) for i in topk_indices]\n","\n","# ì „ì²´ ì‹œìŠ¤í…œ ì‹¤í–‰\n","def find_similar_papers(abstract):\n","    print(\"ðŸ”„ ì´ˆë¡ ìš”ì•½ ì¤‘...\")\n","    summary = summarize_abstract(abstract)\n","    print(f\"ðŸ“ ìš”ì•½ë¬¸: {summary}\")\n","\n","    print(\"ðŸ”„ ìž„ë² ë”© ìƒì„± ì¤‘...\")\n","    query_vec = get_simcse_embedding(summary)\n","\n","    print(\"ðŸ” ìœ ì‚¬ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘...\")\n","    results = retrieve_top_k(query_vec, all_embeddings, all_titles, top_k=20)\n","\n","    print(\"\\nðŸ“š ê°€ìž¥ ìœ ì‚¬í•œ ë…¼ë¬¸ Top-20:\")\n","    for i, (title, score) in enumerate(results, 1):\n","        print(f\"{i:2d}. ({score:.4f}) {title}\")\n"],"metadata":{"id":"wpIOYXYcDaX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# âœ… 8. ì‹¤ì œ ì¶”ë¡ \n","abstract_input = \"\"\"A conflict-free k-coloring of a graph assigns one of k different colors to some of the vertices such that, for every vertex v, there is a color that is assigned to exactly one vertex among v and v's neighbors. Such colorings have applications in wireless networking, robotics, and geometry, and are well-studied in graph theory. Here we study the natural problem of the conflict-free chromatic number chi_CF(G) (the smallest k for which conflict-free k-colorings exist). We provide results both for closed neighborhoods N[v], for which a vertex v is a member of its neighborhood, and for open neighborhoods N(v), for which vertex v is not a member of its neighborhood.\n","For closed neighborhoods, we prove the conflict-free variant of the famous Hadwiger Conjecture: If an arbitrary graph G does not contain K_{k+1} as a minor, then chi_CF(G) <= k. For planar graphs, we obtain a tight worst-case bound: three colors are sometimes necessary and always sufficient. We also give a complete characterization of the computational complexity of conflict-free coloring. Deciding whether chi_CF(G)<= 1 is NP-complete for planar graphs G, but polynomial for outerplanar graphs. Furthermore, deciding whether chi_CF(G)<= 2 is NP-complete for planar graphs G, but always true for outerplanar graphs. For the bicriteria problem of minimizing the number of colored vertices subject to a given bound k on the number of colors, we give a full algorithmic characterization in terms of complexity and approximation for outerplanar and planar graphs.\n","For open neighborhoods, we show that every planar bipartite graph has a conflict-free coloring with at most four colors; on the other hand, we prove that for k in {1,2,3}, it is NP-complete to decide whether a planar bipartite graph has a conflict-free k-coloring. Moreover, we establish that any general} planar graph has a conflict-free coloring with at most eight colors.\"\"\"\n","find_similar_papers(abstract_input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eH75Dl8aLqpq","executionInfo":{"status":"ok","timestamp":1748458372357,"user_tz":-540,"elapsed":4140,"user":{"displayName":"ê¹€ë™ì¤€","userId":"00919442453400366852"}},"outputId":"6be611c4-82db-4b79-91a9-706ec5fe71c1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ”„ ì´ˆë¡ ìš”ì•½ ì¤‘...\n"]},{"output_type":"stream","name":"stderr","text":["Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"]},{"output_type":"stream","name":"stdout","text":["ðŸ“ ìš”ì•½ë¬¸: We show that chi_CF(G)= 1 is NP-complete for planar graphs G, but polynomial for outerplanar graphs. For closed neighborhoods, we show that every planar bipartite graph has a conflict-free coloring with at most four colors; on the other hand, we prove that for k in 1,2,3, it is NPcomplete to decide whether a planar graph with at least four colors has a k-coloring.\n","ðŸ”„ ìž„ë² ë”© ìƒì„± ì¤‘...\n","ðŸ” ìœ ì‚¬ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘...\n","\n","ðŸ“š ê°€ìž¥ ìœ ì‚¬í•œ ë…¼ë¬¸ Top-20:\n"," 1. (0.9937) Conflict-Free Coloring of Planar Graphs\n"," 2. (0.8451) The Parameterized Complexity of Graph Cyclability\n"," 3. (0.8355) Equitable Colorings of $l$-Corona Products of Cubic Graphs\n"," 4. (0.8297) Colouring graphs with constraints on connectivity\n"," 5. (0.8258) Algorithmic Aspects of Regular Graph Covers\n"," 6. (0.8243) On the complete width and edge clique cover problems\n"," 7. (0.8242) Polyhedral studies of vertex coloring problems: The asymmetric\n","  representatives formulation\n"," 8. (0.8223) A note on $\\mathtt{V}$-free $2$-matchings\n"," 9. (0.8208) Rainbow Colouring of Split Graphs\n","10. (0.8189) The complexity of signed graph and edge-coloured graph homomorphisms\n","11. (0.8166) Homomorphism bounds and edge-colourings of $K_4$-minor-free graphs\n","12. (0.8160) Color-line and Proper Color-line Graphs\n","13. (0.8140) A faster FPT Algorithm and a smaller Kernel for Block Graph Vertex\n","  Deletion\n","14. (0.8103) Results for grundy number of the complement of bipartite graphs\n","15. (0.8094) The Maximum k-Differential Coloring Problem\n","16. (0.8088) Fixing improper colorings of graphs\n","17. (0.8086) Models for the k-metric dimension\n","18. (0.8071) Semi-algebraic colorings of complete graphs\n","19. (0.8068) Hadwiger number of graphs with small chordality\n","20. (0.8059) Vertex Cover Gets Faster and Harder on Low Degree Graphs\n"]}]}]}